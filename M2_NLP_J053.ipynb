{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2_NLP_J053.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavito/NLP/blob/master/M2_NLP_J053.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWhPOiy8oEYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "18bc0a17-7fee-4dab-9fce-f49021de2217"
      },
      "source": [
        "!python -m spacy download de"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz#egg=de_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwoQjPFOUgo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import nltk\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import gensim\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.decomposition import NMF\n",
        "import spacy\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1vzDnI9lMmj",
        "colab_type": "code",
        "outputId": "bc7b0108-be5e-4971-cb10-942c76cb55fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZaTav_E4aJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword = set(stopwords.words('german'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sH90nJ2qFZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spac = spacy.load('de')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mc4haBFVAcK",
        "colab_type": "code",
        "outputId": "c4517207-d546-41f6-b80d-80d36fac436d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "!wget https://github.com/amsurve/J050_sem6_nlp/blob/master/datasets/10k-german-news-articles.zip?raw=true"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-03 18:28:42--  https://github.com/amsurve/J050_sem6_nlp/blob/master/datasets/10k-german-news-articles.zip?raw=true\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/amsurve/J050_sem6_nlp/raw/master/datasets/10k-german-news-articles.zip [following]\n",
            "--2020-03-03 18:28:42--  https://github.com/amsurve/J050_sem6_nlp/raw/master/datasets/10k-german-news-articles.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/amsurve/J050_sem6_nlp/master/datasets/10k-german-news-articles.zip [following]\n",
            "--2020-03-03 18:28:42--  https://media.githubusercontent.com/media/amsurve/J050_sem6_nlp/master/datasets/10k-german-news-articles.zip\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128989980 (123M) [application/zip]\n",
            "Saving to: ‘10k-german-news-articles.zip?raw=true’\n",
            "\n",
            "10k-german-news-art 100%[===================>] 123.01M   218MB/s    in 0.6s    \n",
            "\n",
            "2020-03-03 18:28:44 (218 MB/s) - ‘10k-german-news-articles.zip?raw=true’ saved [128989980/128989980]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxChfyFXUoH9",
        "colab_type": "code",
        "outputId": "280df187-3d66-4893-bf51-a517f6727b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "!mkdir newsarticles\n",
        "!unzip 10k-german-news-articles.zip?raw=true -d newsarticles"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  10k-german-news-articles.zip?raw=true\n",
            "  inflating: newsarticles/Annotations.csv  \n",
            "  inflating: newsarticles/Annotations_consolidated.csv  \n",
            "  inflating: newsarticles/Articles.csv  \n",
            "  inflating: newsarticles/Categories.csv  \n",
            "  inflating: newsarticles/CrossValSplit.csv  \n",
            "  inflating: newsarticles/Newspaper_Staff.csv  \n",
            "  inflating: newsarticles/Posts.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gm_4KVUiqOg8",
        "colab": {}
      },
      "source": [
        "zipfile = ZipFile('/content/10k-german-news-articles.zip?raw=true','r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2020ba57-1abd-4a7d-936e-8ba116b4d958",
        "id": "_NLl-6r8qN-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "zipfile.namelist()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Annotations.csv',\n",
              " 'Annotations_consolidated.csv',\n",
              " 'Articles.csv',\n",
              " 'Categories.csv',\n",
              " 'CrossValSplit.csv',\n",
              " 'Newspaper_Staff.csv',\n",
              " 'Posts.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x49nW5vDUpNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anno = pd.read_csv(zipfile.open('Annotations.csv'),header = 'infer')\n",
        "annoconso = pd.read_csv(zipfile.open('Annotations_consolidated.csv'),header = 'infer')\n",
        "articles = pd.read_csv(zipfile.open('Articles.csv'),header = 'infer')\n",
        "categ = pd.read_csv(zipfile.open('Categories.csv'),header = 'infer')\n",
        "crossval = pd.read_csv(zipfile.open('CrossValSplit.csv'),header = 'infer')\n",
        "news = pd.read_csv(zipfile.open('Newspaper_Staff.csv'),header = 'infer')\n",
        "posts = pd.read_csv(zipfile.open('Posts.csv'),header = 'infer')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-XkbyL7ZvMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listallcsv = [anno,annoconso,articles,categ,crossval,news,posts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKsodXJvUpiM",
        "colab_type": "code",
        "outputId": "dfea32b1-167a-44b4-9db2-8a53b91a25ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "#HERE WE FIND OUT ALL THE NULL VALUES PER CSV FILE AND SOME BASIC INFO\n",
        "print('\\nAnno\\n',anno.isna().sum(),anno.columns,anno.shape)\n",
        "print('\\nAnnoConso\\n',annoconso.isna().sum(),annoconso.columns,annoconso.shape)\n",
        "print('\\nArticles\\n',articles.isna().sum(),articles.columns,articles.shape)\n",
        "print('\\nCategories\\n',categ.isna().sum(),categ.columns,categ.shape)\n",
        "print('\\nCrossVal\\n',crossval.isna().sum(),crossval.columns,crossval.shape)\n",
        "print('\\nNews\\n',news.isna().sum(),news.columns,news.shape)\n",
        "print('\\nPosts\\n',posts.isna().sum(),posts.columns,posts.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Anno\n",
            " ID_Post         0\n",
            "ID_Annotator    0\n",
            "Category        0\n",
            "Value           0\n",
            "dtype: int64 Index(['ID_Post', 'ID_Annotator', 'Category', 'Value'], dtype='object') (58568, 4)\n",
            "\n",
            "AnnoConso\n",
            " ID_Post     0\n",
            "Category    0\n",
            "Value       0\n",
            "dtype: int64 Index(['ID_Post', 'Category', 'Value'], dtype='object') (40567, 3)\n",
            "\n",
            "Articles\n",
            " ID_Article        0\n",
            "Path              0\n",
            "publishingDate    0\n",
            "Title             0\n",
            "Body              0\n",
            "dtype: int64 Index(['ID_Article', 'Path', 'publishingDate', 'Title', 'Body'], dtype='object') (12087, 5)\n",
            "\n",
            "Categories\n",
            " Name    0\n",
            "Ord     0\n",
            "dtype: int64 Index(['Name', 'Ord'], dtype='object') (9, 2)\n",
            "\n",
            "CrossVal\n",
            " ID_Post     0\n",
            "Category    0\n",
            "Fold        0\n",
            "dtype: int64 Index(['ID_Post', 'Category', 'Fold'], dtype='object') (40567, 3)\n",
            "\n",
            "News\n",
            " ID_User    0\n",
            "dtype: int64 Index(['ID_User'], dtype='object') (110, 1)\n",
            "\n",
            "Posts\n",
            " ID_Post                0\n",
            "ID_Parent_Post    317007\n",
            "ID_Article             0\n",
            "ID_User                0\n",
            "CreatedAt              0\n",
            "Status                 0\n",
            "Headline          671533\n",
            "Body               49884\n",
            "PositiveVotes          0\n",
            "NegativeVotes          0\n",
            "dtype: int64 Index(['ID_Post', 'ID_Parent_Post', 'ID_Article', 'ID_User', 'CreatedAt',\n",
            "       'Status', 'Headline', 'Body', 'PositiveVotes', 'NegativeVotes'],\n",
            "      dtype='object') (1011773, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u58W3qyDJE6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Merge posts.csv and Annotation_Consolidated.csv\n",
        "result_df = pd.merge(posts, annoconso, on=\"ID_Post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOh1vUNhPhbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#merge the previous df with Articles.csv\n",
        "dfm = pd.merge(result_df, articles, on=\"ID_Article\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzcnK0PXPmi3",
        "colab_type": "code",
        "outputId": "ea06941e-5ebd-4806-f41a-93b0c7b5cf0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "dfm.info()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 40567 entries, 0 to 40566\n",
            "Data columns (total 16 columns):\n",
            "ID_Post           40567 non-null int64\n",
            "ID_Parent_Post    26554 non-null float64\n",
            "ID_Article        40567 non-null int64\n",
            "ID_User           40567 non-null int64\n",
            "CreatedAt         40567 non-null object\n",
            "Status            40567 non-null object\n",
            "Headline          13650 non-null object\n",
            "Body_x            38839 non-null object\n",
            "PositiveVotes     40567 non-null int64\n",
            "NegativeVotes     40567 non-null int64\n",
            "Category          40567 non-null object\n",
            "Value             40567 non-null int64\n",
            "Path              40567 non-null object\n",
            "publishingDate    40567 non-null object\n",
            "Title             40567 non-null object\n",
            "Body_y            40567 non-null object\n",
            "dtypes: float64(1), int64(6), object(9)\n",
            "memory usage: 5.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7abBueYCamPx",
        "colab_type": "code",
        "outputId": "3ed02f12-a064-4081-c051-e815c84bf522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#As given in Category.csv, every category has a specific order of priority, so replace category name with order\n",
        "for j in range(dfm.shape[0]):\n",
        "  for i in range(categ.shape[0]):\n",
        "    if dfm['Category'][j]==categ['Name'][i]:\n",
        "      dfm['Category'][j]=categ['Ord'][i]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adKkNBTxR4cQ",
        "colab_type": "code",
        "outputId": "f77d9f1e-9045-4919-96da-d214ba327ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dfm #IS OUR DENORMALISED dataset with all necessary columns\n",
        "#CSVs used until now: Posts, Annotation, Articles ,Category"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_Post</th>\n",
              "      <th>ID_Parent_Post</th>\n",
              "      <th>ID_Article</th>\n",
              "      <th>ID_User</th>\n",
              "      <th>CreatedAt</th>\n",
              "      <th>Status</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body_x</th>\n",
              "      <th>PositiveVotes</th>\n",
              "      <th>NegativeVotes</th>\n",
              "      <th>Category</th>\n",
              "      <th>Value</th>\n",
              "      <th>Path</th>\n",
              "      <th>publishingDate</th>\n",
              "      <th>Title</th>\n",
              "      <th>Body_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>79</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>12071</td>\n",
              "      <td>2015-06-01 08:58:32.363</td>\n",
              "      <td>online</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ich kann keinen hinweis finden, wo man sich hi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Newsroom/User/Community</td>\n",
              "      <td>2012-05-26 03:00:19.23</td>\n",
              "      <td>Die Newsletter von derStandard.at</td>\n",
              "      <td>&lt;div class=\"section\" id=\"content-main\" itempro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>81</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>27465</td>\n",
              "      <td>2015-10-03 14:06:06.210</td>\n",
              "      <td>online</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sehr geehrte Community oder cmb!\\r\\n\\r\\nBekomm...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Newsroom/User/Community</td>\n",
              "      <td>2012-05-26 03:00:19.23</td>\n",
              "      <td>Die Newsletter von derStandard.at</td>\n",
              "      <td>&lt;div class=\"section\" id=\"content-main\" itempro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>132</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>28073</td>\n",
              "      <td>2015-10-09 11:33:07.227</td>\n",
              "      <td>online</td>\n",
              "      <td>Irgendwie...</td>\n",
              "      <td>kommen manche Posts nicht durch :(</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Newsroom/User/Community/Regeln</td>\n",
              "      <td>2012-05-26 12:12:19.46</td>\n",
              "      <td>Werden Sie Teil von derStandard.at!</td>\n",
              "      <td>&lt;div class=\"diashow\" id=\"objectContent\"&gt;&lt;meta ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>134</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>20194</td>\n",
              "      <td>2015-11-04 13:29:34.930</td>\n",
              "      <td>online</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eine Frage: \\r\\n\\r\\nGibt es eine Möglichkeit s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Newsroom/User/Community/Regeln</td>\n",
              "      <td>2012-05-26 12:12:19.46</td>\n",
              "      <td>Werden Sie Teil von derStandard.at!</td>\n",
              "      <td>&lt;div class=\"diashow\" id=\"objectContent\"&gt;&lt;meta ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>139</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>14020</td>\n",
              "      <td>2015-12-11 14:51:17.540</td>\n",
              "      <td>online</td>\n",
              "      <td>Registrierung</td>\n",
              "      <td>Ich habe ein Problem: ich bin registriert, übe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Newsroom/User/Community/Regeln</td>\n",
              "      <td>2012-05-26 12:12:19.46</td>\n",
              "      <td>Werden Sie Teil von derStandard.at!</td>\n",
              "      <td>&lt;div class=\"diashow\" id=\"objectContent\"&gt;&lt;meta ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40562</th>\n",
              "      <td>1006960</td>\n",
              "      <td>1006958.0</td>\n",
              "      <td>12070</td>\n",
              "      <td>24704</td>\n",
              "      <td>2016-05-31 16:37:21.110</td>\n",
              "      <td>online</td>\n",
              "      <td>Früher</td>\n",
              "      <td>haben sie ein vierteltelefon beantragen dürfen...</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Newsroom/Web/Telekommunikation/mobilc</td>\n",
              "      <td>2016-05-31 14:37:48.00</td>\n",
              "      <td>Neue A1-Chefin Schramböck rührt im Management um</td>\n",
              "      <td>&lt;div class=\"section\" id=\"content-main\" itempro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40563</th>\n",
              "      <td>1006960</td>\n",
              "      <td>1006958.0</td>\n",
              "      <td>12070</td>\n",
              "      <td>24704</td>\n",
              "      <td>2016-05-31 16:37:21.110</td>\n",
              "      <td>online</td>\n",
              "      <td>Früher</td>\n",
              "      <td>haben sie ein vierteltelefon beantragen dürfen...</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Newsroom/Web/Telekommunikation/mobilc</td>\n",
              "      <td>2016-05-31 14:37:48.00</td>\n",
              "      <td>Neue A1-Chefin Schramböck rührt im Management um</td>\n",
              "      <td>&lt;div class=\"section\" id=\"content-main\" itempro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40564</th>\n",
              "      <td>1006960</td>\n",
              "      <td>1006958.0</td>\n",
              "      <td>12070</td>\n",
              "      <td>24704</td>\n",
              "      <td>2016-05-31 16:37:21.110</td>\n",
              "      <td>online</td>\n",
              "      <td>Früher</td>\n",
              "      <td>haben sie ein vierteltelefon beantragen dürfen...</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Newsroom/Web/Telekommunikation/mobilc</td>\n",
              "      <td>2016-05-31 14:37:48.00</td>\n",
              "      <td>Neue A1-Chefin Schramböck rührt im Management um</td>\n",
              "      <td>&lt;div class=\"section\" id=\"content-main\" itempro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40565</th>\n",
              "      <td>1010868</td>\n",
              "      <td>1010862.0</td>\n",
              "      <td>12079</td>\n",
              "      <td>14509</td>\n",
              "      <td>2016-05-31 18:24:10.780</td>\n",
              "      <td>online</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In dem Artikel, unter dem du dein post abgeset...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>Newsroom/Web/Innovationen/Microsoft</td>\n",
              "      <td>2016-05-31 09:43:18.00</td>\n",
              "      <td>Windows-10-Upgrade: Microsoft will \"üble Trick...</td>\n",
              "      <td>&lt;div class=\"section\" id=\"content-main\" itempro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40566</th>\n",
              "      <td>1010997</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12084</td>\n",
              "      <td>576</td>\n",
              "      <td>2016-05-31 17:54:24.000</td>\n",
              "      <td>online</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Die User haben hier, haben es doch oft formuli...</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Newsroom/Etat/PRINT/Springer</td>\n",
              "      <td>2016-05-31 17:39:29.00</td>\n",
              "      <td>\"Können Adblocker nicht einfach hinnehmen\"</td>\n",
              "      <td>&lt;div class=\"section\" id=\"content-main\" itempro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40567 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID_Post  ...                                             Body_y\n",
              "0           79  ...  <div class=\"section\" id=\"content-main\" itempro...\n",
              "1           81  ...  <div class=\"section\" id=\"content-main\" itempro...\n",
              "2          132  ...  <div class=\"diashow\" id=\"objectContent\"><meta ...\n",
              "3          134  ...  <div class=\"diashow\" id=\"objectContent\"><meta ...\n",
              "4          139  ...  <div class=\"diashow\" id=\"objectContent\"><meta ...\n",
              "...        ...  ...                                                ...\n",
              "40562  1006960  ...  <div class=\"section\" id=\"content-main\" itempro...\n",
              "40563  1006960  ...  <div class=\"section\" id=\"content-main\" itempro...\n",
              "40564  1006960  ...  <div class=\"section\" id=\"content-main\" itempro...\n",
              "40565  1010868  ...  <div class=\"section\" id=\"content-main\" itempro...\n",
              "40566  1010997  ...  <div class=\"section\" id=\"content-main\" itempro...\n",
              "\n",
              "[40567 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUKqkEBVSDkc",
        "colab_type": "code",
        "outputId": "5e0718e6-f0ee-4417-eb86-747861793c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "dfm.info()#We Now Extract Topics from Title and Body and compare"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 40567 entries, 0 to 40566\n",
            "Data columns (total 16 columns):\n",
            "ID_Post           40567 non-null int64\n",
            "ID_Parent_Post    26554 non-null float64\n",
            "ID_Article        40567 non-null int64\n",
            "ID_User           40567 non-null int64\n",
            "CreatedAt         40567 non-null object\n",
            "Status            40567 non-null object\n",
            "Headline          13650 non-null object\n",
            "Body_x            38839 non-null object\n",
            "PositiveVotes     40567 non-null int64\n",
            "NegativeVotes     40567 non-null int64\n",
            "Category          40567 non-null object\n",
            "Value             40567 non-null int64\n",
            "Path              40567 non-null object\n",
            "publishingDate    40567 non-null object\n",
            "Title             40567 non-null object\n",
            "Body_y            40567 non-null object\n",
            "dtypes: float64(1), int64(6), object(9)\n",
            "memory usage: 6.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUCT6v7mZHka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove html tags\n",
        "def remove_html(x):\n",
        "  return BeautifulSoup(x, \"lxml\").text\n",
        "\n",
        "#remove \\n from sentences\n",
        "def remove_slash_n(x):\n",
        "  return re.sub('\\n', \" \",x)\n",
        "\n",
        "#basic cleaning\n",
        "def clean_text(x):\n",
        "  x = x.rstrip()\n",
        "  sentence = x.lower()\n",
        "  S=re.sub('\"','',sentence)\n",
        "  S = re.sub(r'[^\\w]', ' ', S)\n",
        "  S=re.sub('[^a-zA-Z]', \" \", S)\n",
        "  stop_free = ' '.join([word for word in S.lower().split() if ((word not in stopword))])\n",
        "  return stop_free\n",
        "\n",
        "#german lemmatizer spacy\n",
        "def lemmatazer(x):\n",
        "  sentence = ' '.join([t.lemma_ for t in spac.tokenizer(x)])\n",
        "  return sentence\n",
        "\n",
        "#culmination of all\n",
        "def preprocessing_text(x):\n",
        "  x=remove_html(x)\n",
        "  x=remove_slash_n(x)\n",
        "  x=clean_text(x)\n",
        "  x=lemmatazer(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m89Ti1XXqCCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#applying text preprocessing to required columns\n",
        "dfm['CleanBody'] = dfm['Body_y'].apply(preprocessing_text)\n",
        "dfm['CleanTitle'] = dfm['Title'].apply(preprocessing_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzMPv8Ce_K5L",
        "colab_type": "text"
      },
      "source": [
        "#TOPIC MODELLING DONE BY NMF INSTEAD OF LDA, PROVIDES BETTER RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEoWTOTv7iz_",
        "colab_type": "text"
      },
      "source": [
        "FOR TITLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7psNbUdWVpzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "message=[]\n",
        "message2=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eCxbobMm18A1",
        "colab": {}
      },
      "source": [
        "vectorizer1 = TfidfVectorizer()\n",
        "X = vectorizer1.fit_transform(dfm['CleanTitle'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RAKiu6KN17vl",
        "colab": {}
      },
      "source": [
        "idx_to_word1 = np.array(vectorizer1.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_qoSBi62t2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmf1 = NMF(n_components=20, solver=\"mu\")\n",
        "W1 = nmf1.fit_transform(X)\n",
        "H1 = nmf1.components_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_j-NMwz22L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, topic in enumerate(H1):\n",
        "  message.append([str(x) for x in idx_to_word1[topic.argsort()[-10:]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vcxfdjsI-at9"
      },
      "source": [
        "FOR BODY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMc42b2w_sN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer2 = TfidfVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j_zXK1_c-a6g",
        "colab": {}
      },
      "source": [
        "Y = vectorizer2.fit_transform(dfm['CleanBody'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CNBj5WQ_-bFC",
        "colab": {}
      },
      "source": [
        "idx_to_word2 = np.array(vectorizer2.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RIDorSmS-bY9",
        "colab": {}
      },
      "source": [
        "nmf2 = NMF(n_components=20, solver=\"mu\")\n",
        "W2 = nmf2.fit_transform(Y)\n",
        "H2 = nmf2.components_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67PI9n2N-bsO",
        "colab": {}
      },
      "source": [
        "for i, topic in enumerate(H2):\n",
        "    message2.append([str(x) for x in idx_to_word2[topic.argsort()[-10:]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Aqa96HK-tTf",
        "colab_type": "text"
      },
      "source": [
        "#SIMILARITY BETWEEN TITLE AND BODY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgQrXXFzJgEq",
        "colab_type": "text"
      },
      "source": [
        "FOR BODY and title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P11jNfiNMfrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "message=pd.DataFrame(message).add_prefix('word_')\n",
        "message2=pd.DataFrame(message2).add_prefix('word_')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTQ2SIu6WH9N",
        "colab_type": "code",
        "outputId": "e0a2bba2-b0ac-4c0a-f9c3-91bbad6c7c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "message"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_0</th>\n",
              "      <th>word_1</th>\n",
              "      <th>word_2</th>\n",
              "      <th>word_3</th>\n",
              "      <th>word_4</th>\n",
              "      <th>word_5</th>\n",
              "      <th>word_6</th>\n",
              "      <th>word_7</th>\n",
              "      <th>word_8</th>\n",
              "      <th>word_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>modeimperiums</td>\n",
              "      <td>wunsch</td>\n",
              "      <td>lehrstellen</td>\n",
              "      <td>fu</td>\n",
              "      <td>chtling</td>\n",
              "      <td>balltrainer</td>\n",
              "      <td>fall</td>\n",
              "      <td>realit</td>\n",
              "      <td>tsverlust</td>\n",
              "      <td>genderwahn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>babys</td>\n",
              "      <td>schlafen</td>\n",
              "      <td>gen</td>\n",
              "      <td>slowakische</td>\n",
              "      <td>kleinstadt</td>\n",
              "      <td>kopftuchkonflikte</td>\n",
              "      <td>traiskirchen</td>\n",
              "      <td>iran</td>\n",
              "      <td>kinderarbeiter</td>\n",
              "      <td>zeltbewohner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>verbilligen</td>\n",
              "      <td>einsperren</td>\n",
              "      <td>fl</td>\n",
              "      <td>chtlinge</td>\n",
              "      <td>erneut</td>\n",
              "      <td>polizei</td>\n",
              "      <td>setzen</td>\n",
              "      <td>idomeni</td>\n",
              "      <td>tr</td>\n",
              "      <td>nengas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stehen</td>\n",
              "      <td>reisen</td>\n",
              "      <td>entl</td>\n",
              "      <td>pal</td>\n",
              "      <td>sst</td>\n",
              "      <td>israel</td>\n",
              "      <td>frei</td>\n",
              "      <td>ngste</td>\n",
              "      <td>stinensische</td>\n",
              "      <td>gefangen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>griss</td>\n",
              "      <td>upl</td>\n",
              "      <td>ball</td>\n",
              "      <td>blatter</td>\n",
              "      <td>alternativ</td>\n",
              "      <td>fu</td>\n",
              "      <td>warum</td>\n",
              "      <td>bleiben</td>\n",
              "      <td>gewalt</td>\n",
              "      <td>partnerschaft</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>eisenstange</td>\n",
              "      <td>frau</td>\n",
              "      <td>pal</td>\n",
              "      <td>israelisch</td>\n",
              "      <td>verletzen</td>\n",
              "      <td>tet</td>\n",
              "      <td>get</td>\n",
              "      <td>soldat</td>\n",
              "      <td>stinenser</td>\n",
              "      <td>kopfschuss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>chtigen</td>\n",
              "      <td>tatverd</td>\n",
              "      <td>erst</td>\n",
              "      <td>gest</td>\n",
              "      <td>ndnis</td>\n",
              "      <td>praterstern</td>\n",
              "      <td>vergewaltigung</td>\n",
              "      <td>ftling</td>\n",
              "      <td>vorfall</td>\n",
              "      <td>verlegen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>neuformulierung</td>\n",
              "      <td>euro</td>\n",
              "      <td>millionen</td>\n",
              "      <td>bekommen</td>\n",
              "      <td>new</td>\n",
              "      <td>york</td>\n",
              "      <td>schwedin</td>\n",
              "      <td>sexuell</td>\n",
              "      <td>stigung</td>\n",
              "      <td>bel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>mariahilfer</td>\n",
              "      <td>bau</td>\n",
              "      <td>abschließen</td>\n",
              "      <td>zufrieden</td>\n",
              "      <td>israelisch</td>\n",
              "      <td>neu</td>\n",
              "      <td>gesetz</td>\n",
              "      <td>recht</td>\n",
              "      <td>siedlungen</td>\n",
              "      <td>ausdehnen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>deutschland</td>\n",
              "      <td>sterreich</td>\n",
              "      <td>tickets</td>\n",
              "      <td>einstellen</td>\n",
              "      <td>verbilligen</td>\n",
              "      <td>fl</td>\n",
              "      <td>chtlinge</td>\n",
              "      <td>junge</td>\n",
              "      <td>angeblich</td>\n",
              "      <td>egoisten</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>fu</td>\n",
              "      <td>chtling</td>\n",
              "      <td>balltrainer</td>\n",
              "      <td>postings</td>\n",
              "      <td>fleisch</td>\n",
              "      <td>brauerei</td>\n",
              "      <td>geworden</td>\n",
              "      <td>gut</td>\n",
              "      <td>leben</td>\n",
              "      <td>job</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>kurdenmiliz</td>\n",
              "      <td>unm</td>\n",
              "      <td>gehen</td>\n",
              "      <td>weihnachten</td>\n",
              "      <td>traurig</td>\n",
              "      <td>nder</td>\n",
              "      <td>linksh</td>\n",
              "      <td>arbeitssuche</td>\n",
              "      <td>machen</td>\n",
              "      <td>erfahrungen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>asked</td>\n",
              "      <td>frequently</td>\n",
              "      <td>questions</td>\n",
              "      <td>postings</td>\n",
              "      <td>nderungen</td>\n",
              "      <td>kennzeichnung</td>\n",
              "      <td>community</td>\n",
              "      <td>off</td>\n",
              "      <td>topic</td>\n",
              "      <td>forum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>markt</td>\n",
              "      <td>unfertig</td>\n",
              "      <td>produkte</td>\n",
              "      <td>sterreich</td>\n",
              "      <td>ge</td>\n",
              "      <td>asylantr</td>\n",
              "      <td>ns</td>\n",
              "      <td>orb</td>\n",
              "      <td>jahr</td>\n",
              "      <td>erleben</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>vergewaltigung</td>\n",
              "      <td>gest</td>\n",
              "      <td>ndnis</td>\n",
              "      <td>mal</td>\n",
              "      <td>world</td>\n",
              "      <td>wide</td>\n",
              "      <td>web</td>\n",
              "      <td>geschichte</td>\n",
              "      <td>auto</td>\n",
              "      <td>erst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>dieselmotoren</td>\n",
              "      <td>geben</td>\n",
              "      <td>syrien</td>\n",
              "      <td>grund</td>\n",
              "      <td>kurzherbergen</td>\n",
              "      <td>rasend</td>\n",
              "      <td>familie</td>\n",
              "      <td>betreffen</td>\n",
              "      <td>fluchen</td>\n",
              "      <td>heimat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>elf</td>\n",
              "      <td>jahrzehnten</td>\n",
              "      <td>rre</td>\n",
              "      <td>thiopien</td>\n",
              "      <td>viral</td>\n",
              "      <td>it</td>\n",
              "      <td>mitarbeitern</td>\n",
              "      <td>erlebnisse</td>\n",
              "      <td>schlimm</td>\n",
              "      <td>hotelerlebnisse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>viraler</td>\n",
              "      <td>spenden</td>\n",
              "      <td>werden</td>\n",
              "      <td>blut</td>\n",
              "      <td>abweisen</td>\n",
              "      <td>trotzen</td>\n",
              "      <td>gesetzes</td>\n",
              "      <td>fortpflanzungsmedizin</td>\n",
              "      <td>kind</td>\n",
              "      <td>wollen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>heikle</td>\n",
              "      <td>ne</td>\n",
              "      <td>gehen</td>\n",
              "      <td>traurig</td>\n",
              "      <td>weihnachten</td>\n",
              "      <td>machen</td>\n",
              "      <td>pl</td>\n",
              "      <td>lohn</td>\n",
              "      <td>tzlich</td>\n",
              "      <td>erfahrung</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>griss</td>\n",
              "      <td>spenden</td>\n",
              "      <td>kampagne</td>\n",
              "      <td>bund</td>\n",
              "      <td>erzwingen</td>\n",
              "      <td>asylquartiere</td>\n",
              "      <td>gemeinden</td>\n",
              "      <td>erste</td>\n",
              "      <td>urlaub</td>\n",
              "      <td>alleine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             word_0       word_1  ...          word_8           word_9\n",
              "0     modeimperiums       wunsch  ...       tsverlust       genderwahn\n",
              "1             babys     schlafen  ...  kinderarbeiter     zeltbewohner\n",
              "2       verbilligen   einsperren  ...              tr           nengas\n",
              "3            stehen       reisen  ...    stinensische         gefangen\n",
              "4             griss          upl  ...          gewalt    partnerschaft\n",
              "5       eisenstange         frau  ...       stinenser       kopfschuss\n",
              "6           chtigen      tatverd  ...         vorfall         verlegen\n",
              "7   neuformulierung         euro  ...         stigung              bel\n",
              "8       mariahilfer          bau  ...      siedlungen        ausdehnen\n",
              "9       deutschland    sterreich  ...       angeblich         egoisten\n",
              "10               fu      chtling  ...           leben              job\n",
              "11      kurdenmiliz          unm  ...          machen      erfahrungen\n",
              "12            asked   frequently  ...           topic            forum\n",
              "13            markt     unfertig  ...            jahr          erleben\n",
              "14   vergewaltigung         gest  ...            auto             erst\n",
              "15    dieselmotoren        geben  ...         fluchen           heimat\n",
              "16              elf  jahrzehnten  ...         schlimm  hotelerlebnisse\n",
              "17          viraler      spenden  ...            kind           wollen\n",
              "18           heikle           ne  ...          tzlich        erfahrung\n",
              "19            griss      spenden  ...          urlaub          alleine\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ac8DYFoqu3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "e0b712b3-393a-4799-c95a-63d4c99e4c90"
      },
      "source": [
        "message2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_0</th>\n",
              "      <th>word_1</th>\n",
              "      <th>word_2</th>\n",
              "      <th>word_3</th>\n",
              "      <th>word_4</th>\n",
              "      <th>word_5</th>\n",
              "      <th>word_6</th>\n",
              "      <th>word_7</th>\n",
              "      <th>word_8</th>\n",
              "      <th>word_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hausbichler</td>\n",
              "      <td>matriarchats</td>\n",
              "      <td>zeigen</td>\n",
              "      <td>laut</td>\n",
              "      <td>scheinen</td>\n",
              "      <td>ngst</td>\n",
              "      <td>ltnisse</td>\n",
              "      <td>feminismus</td>\n",
              "      <td>tun</td>\n",
              "      <td>kritik</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>acht</td>\n",
              "      <td>richtig</td>\n",
              "      <td>junge</td>\n",
              "      <td>asylwerber</td>\n",
              "      <td>hriger</td>\n",
              "      <td>afghanistan</td>\n",
              "      <td>iran</td>\n",
              "      <td>sami</td>\n",
              "      <td>chte</td>\n",
              "      <td>zelt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mazedonisch</td>\n",
              "      <td>hilfen</td>\n",
              "      <td>rkei</td>\n",
              "      <td>versuchen</td>\n",
              "      <td>grenze</td>\n",
              "      <td>sein</td>\n",
              "      <td>mitrow</td>\n",
              "      <td>eu</td>\n",
              "      <td>chtlinge</td>\n",
              "      <td>fl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>zumeist</td>\n",
              "      <td>jemals</td>\n",
              "      <td>wawi</td>\n",
              "      <td>strafm</td>\n",
              "      <td>milit</td>\n",
              "      <td>hrige</td>\n",
              "      <td>stinensische</td>\n",
              "      <td>lfj</td>\n",
              "      <td>zw</td>\n",
              "      <td>pal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fragen</td>\n",
              "      <td>beziehung</td>\n",
              "      <td>frauenh</td>\n",
              "      <td>warum</td>\n",
              "      <td>frauen</td>\n",
              "      <td>partner</td>\n",
              "      <td>tigen</td>\n",
              "      <td>betroffen</td>\n",
              "      <td>gewaltt</td>\n",
              "      <td>gewalt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ver</td>\n",
              "      <td>kopf</td>\n",
              "      <td>ffentlicht</td>\n",
              "      <td>armee</td>\n",
              "      <td>menschenrechtsorganisation</td>\n",
              "      <td>video</td>\n",
              "      <td>verletzen</td>\n",
              "      <td>israelisch</td>\n",
              "      <td>betselem</td>\n",
              "      <td>soldat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>apa</td>\n",
              "      <td>jugendlich</td>\n",
              "      <td>attacke</td>\n",
              "      <td>freitag</td>\n",
              "      <td>nieder</td>\n",
              "      <td>martin</td>\n",
              "      <td>verlegen</td>\n",
              "      <td>justizministerium</td>\n",
              "      <td>praterstern</td>\n",
              "      <td>tichy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>new</td>\n",
              "      <td>gericht</td>\n",
              "      <td>gesp</td>\n",
              "      <td>york</td>\n",
              "      <td>digung</td>\n",
              "      <td>entsch</td>\n",
              "      <td>finanzen</td>\n",
              "      <td>schwedin</td>\n",
              "      <td>chef</td>\n",
              "      <td>millionen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>spielen</td>\n",
              "      <td>gewinnen</td>\n",
              "      <td>spa</td>\n",
              "      <td>deutschen</td>\n",
              "      <td>ikbal</td>\n",
              "      <td>fu</td>\n",
              "      <td>grenzen</td>\n",
              "      <td>kicken</td>\n",
              "      <td>gst</td>\n",
              "      <td>ttner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>arbeit</td>\n",
              "      <td>jahr</td>\n",
              "      <td>verm</td>\n",
              "      <td>netto</td>\n",
              "      <td>poor</td>\n",
              "      <td>working</td>\n",
              "      <td>besteuern</td>\n",
              "      <td>wer</td>\n",
              "      <td>trotzen</td>\n",
              "      <td>mittelschicht</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>stinensischen</td>\n",
              "      <td>hintert</td>\n",
              "      <td>knesset</td>\n",
              "      <td>justizministerin</td>\n",
              "      <td>bennett</td>\n",
              "      <td>rverwaltung</td>\n",
              "      <td>pal</td>\n",
              "      <td>vorhaben</td>\n",
              "      <td>annexion</td>\n",
              "      <td>siedlungen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>nger</td>\n",
              "      <td>zahl</td>\n",
              "      <td>arbeitslos</td>\n",
              "      <td>erwerbst</td>\n",
              "      <td>besch</td>\n",
              "      <td>job</td>\n",
              "      <td>ftigte</td>\n",
              "      <td>ltere</td>\n",
              "      <td>pension</td>\n",
              "      <td>kompromisse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>vorgesch</td>\n",
              "      <td>topic</td>\n",
              "      <td>otf</td>\n",
              "      <td>gelten</td>\n",
              "      <td>themen</td>\n",
              "      <td>forum</td>\n",
              "      <td>diskussion</td>\n",
              "      <td>community</td>\n",
              "      <td>gegenstand</td>\n",
              "      <td>postings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>at</td>\n",
              "      <td>schick</td>\n",
              "      <td>geschichten</td>\n",
              "      <td>instagram</td>\n",
              "      <td>allt</td>\n",
              "      <td>erinnerungen</td>\n",
              "      <td>umbr</td>\n",
              "      <td>unordnung</td>\n",
              "      <td>derstandard</td>\n",
              "      <td>jahr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>erwachsenwerdens</td>\n",
              "      <td>roadtrip</td>\n",
              "      <td>hrerscheinneulingen</td>\n",
              "      <td>erfahrungendas</td>\n",
              "      <td>macken</td>\n",
              "      <td>trinkend</td>\n",
              "      <td>viel</td>\n",
              "      <td>freiheit</td>\n",
              "      <td>auto</td>\n",
              "      <td>erste</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>umherwandern</td>\n",
              "      <td>urlaubsbeschwerden</td>\n",
              "      <td>urlaubsfreuden</td>\n",
              "      <td>campingplatz</td>\n",
              "      <td>erboste</td>\n",
              "      <td>kopfhaar</td>\n",
              "      <td>nacht</td>\n",
              "      <td>strand</td>\n",
              "      <td>urlauberbeschwerden</td>\n",
              "      <td>urlaub</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>vorstellen</td>\n",
              "      <td>geschichte</td>\n",
              "      <td>nachrichten</td>\n",
              "      <td>beinhalten</td>\n",
              "      <td>familiengeschichten</td>\n",
              "      <td>fluchen</td>\n",
              "      <td>sterreich</td>\n",
              "      <td>heimat</td>\n",
              "      <td>chtlingsland</td>\n",
              "      <td>fl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>traumberuf</td>\n",
              "      <td>hirngespinste</td>\n",
              "      <td>bedruckt</td>\n",
              "      <td>reiterin</td>\n",
              "      <td>brotlose</td>\n",
              "      <td>berufsw</td>\n",
              "      <td>kollegenschaft</td>\n",
              "      <td>gar</td>\n",
              "      <td>spa</td>\n",
              "      <td>kollegin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>arbeiten</td>\n",
              "      <td>nde</td>\n",
              "      <td>derstandard</td>\n",
              "      <td>greifen</td>\n",
              "      <td>alltag</td>\n",
              "      <td>erledigen</td>\n",
              "      <td>rechts</td>\n",
              "      <td>rechtsh</td>\n",
              "      <td>nder</td>\n",
              "      <td>linksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>unternehmens</td>\n",
              "      <td>arbeitgeber</td>\n",
              "      <td>lter</td>\n",
              "      <td>insolvenz</td>\n",
              "      <td>liquidit</td>\n",
              "      <td>sparpakete</td>\n",
              "      <td>schwierigkeiten</td>\n",
              "      <td>wirtschaftlich</td>\n",
              "      <td>pleite</td>\n",
              "      <td>zielpunkt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              word_0              word_1  ...               word_8         word_9\n",
              "0        hausbichler        matriarchats  ...                  tun         kritik\n",
              "1               acht             richtig  ...                 chte           zelt\n",
              "2        mazedonisch              hilfen  ...             chtlinge             fl\n",
              "3            zumeist              jemals  ...                   zw            pal\n",
              "4             fragen           beziehung  ...              gewaltt         gewalt\n",
              "5                ver                kopf  ...             betselem         soldat\n",
              "6                apa          jugendlich  ...          praterstern          tichy\n",
              "7                new             gericht  ...                 chef      millionen\n",
              "8            spielen            gewinnen  ...                  gst          ttner\n",
              "9             arbeit                jahr  ...              trotzen  mittelschicht\n",
              "10     stinensischen             hintert  ...             annexion     siedlungen\n",
              "11              nger                zahl  ...              pension    kompromisse\n",
              "12          vorgesch               topic  ...           gegenstand       postings\n",
              "13                at              schick  ...          derstandard           jahr\n",
              "14  erwachsenwerdens            roadtrip  ...                 auto          erste\n",
              "15      umherwandern  urlaubsbeschwerden  ...  urlauberbeschwerden         urlaub\n",
              "16        vorstellen          geschichte  ...         chtlingsland             fl\n",
              "17        traumberuf       hirngespinste  ...                  spa       kollegin\n",
              "18          arbeiten                 nde  ...                 nder         linksh\n",
              "19      unternehmens         arbeitgeber  ...               pleite      zielpunkt\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Cnxy3XWW7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "3a97f648-76e5-4b40-ff03-5542a767cb58"
      },
      "source": [
        "j = 0\n",
        "for i in range(10):\n",
        "  topic_title = \" \".join(message.values[i])\n",
        "  topic_body = \" \".join(message2.values[i])\n",
        "\n",
        "  ttt = spac(topic_title)\n",
        "  ttb = spac(topic_body)\n",
        "  print(ttt.similarity(ttb))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8492715003521002\n",
            "0.7073257208685212\n",
            "0.880438770822803\n",
            "0.7337482763674271\n",
            "0.7817328936946805\n",
            "0.8328940661922916\n",
            "0.8183525717800229\n",
            "0.8982939901568343\n",
            "0.6830815138251166\n",
            "0.6812740005250405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSZZyXiaYJ50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messagel = [' '.join(message.values[i]) for i in range(len(message))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJEvF0XVXWGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messagel2 = [' '.join(message2.values[i]) for i in range(len(message2))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BflQ6ddYxsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ed14a3a-c6a7-4290-aab2-1a6666b5425a"
      },
      "source": [
        "for i in range(len(message)):\n",
        "  ttt = spac(messagel[i])\n",
        "  ttb = spac(messagel2[i])\n",
        "  print('Similarity % : ',ttt.similarity(ttb))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Similarity % :  0.8492715003521002\n",
            "Similarity % :  0.7073257208685212\n",
            "Similarity % :  0.880438770822803\n",
            "Similarity % :  0.7337482763674271\n",
            "Similarity % :  0.7817328936946805\n",
            "Similarity % :  0.8328940661922916\n",
            "Similarity % :  0.8183525717800229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Similarity % :  0.8982939901568343\n",
            "Similarity % :  0.6830815138251166\n",
            "Similarity % :  0.6812740005250405\n",
            "Similarity % :  0.7427722219601893\n",
            "Similarity % :  0.8426403050652811\n",
            "Similarity % :  0.8766047184837708\n",
            "Similarity % :  0.7816973647778667\n",
            "Similarity % :  0.7429175426283856\n",
            "Similarity % :  0.8102325536286152\n",
            "Similarity % :  0.7524006083487659\n",
            "Similarity % :  0.6037463718701144\n",
            "Similarity % :  0.7970408736642379\n",
            "Similarity % :  0.9033150572148319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwwhme-ouow",
        "colab_type": "text"
      },
      "source": [
        "#Sheet Done, M2 Done , F"
      ]
    }
  ]
}